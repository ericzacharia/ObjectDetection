{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udhh5dN_Ai54",
        "outputId": "b9fb472b-9ace-4a97-8685-0a1dfb798f3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/eric/Desktop/2-Career/Projects/ObjectDetection'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import supervision as sv\n",
        "from autodistill.detection import CaptionOntology\n",
        "from autodistill_grounded_sam import GroundedSAM\n",
        "# from autodistill_yolov8 import YOLOv8\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import video_multiprocessing as vm\n",
        "import superimpose_boxes as sb\n",
        "\n",
        "HOME = os.getcwd()\n",
        "HOME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "project = 'dog_park'\n",
        "# project = 'license_plates'\n",
        "# project = 'hockey'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "VIDEO_DIR_PATH = f\"{HOME}/{project}/videos\"\n",
        "IMAGE_DIR_PATH = f\"{HOME}/{project}/images\"\n",
        "DATASET_DIR_PATH = f\"{HOME}/{project}/dataset\"\n",
        "DATASET_TRAIN_DIR_PATH = f\"{DATASET_DIR_PATH}/train\"\n",
        "DATASET_VALID_DIR_PATH = f\"{DATASET_DIR_PATH}/valid\"\n",
        "os.makedirs(DATASET_DIR_PATH, exist_ok=True)\n",
        "os.makedirs(DATASET_TRAIN_DIR_PATH, exist_ok=True)\n",
        "os.makedirs(DATASET_VALID_DIR_PATH, exist_ok=True)\n",
        "ANNOTATIONS_TRAIN_DIRECTORY_PATH = f\"{DATASET_TRAIN_DIR_PATH}/labels\"\n",
        "ANNOTATIONS_VALID_DIRECTORY_PATH = f\"{DATASET_VALID_DIR_PATH}/labels\"\n",
        "IMAGES_TRAIN_DIRECTORY_PATH = f\"{DATASET_TRAIN_DIR_PATH}/images\"\n",
        "IMAGES_VALID_DIRECTORY_PATH = f\"{DATASET_VALID_DIR_PATH}/images\"\n",
        "SAM_DATA_YAML_PATH = f\"{DATASET_DIR_PATH}/data.yaml\" # sam_data_yaml includes more labels than yolo_data_yaml to assist SAM with detecting license plates\n",
        "YOLO_DATA_YAML_PATH = f\"{DATASET_DIR_PATH}/data.yaml\" # yolo_data_yaml includes the consolidated labels for the yolo model training\n",
        "ANNOTATIONS_TRAIN_FILTERED_DIRECTORY_PATH = f\"{DATASET_TRAIN_DIR_PATH}/labels_filtered\"\n",
        "ANNOTATIONS_VALID_FILTERED_DIRECTORY_PATH = f\"{DATASET_VALID_DIR_PATH}/labels_filtered\"\n",
        "SUPERIMPOSED_ANNOTATIONS_TRAIN_DIRECTORY_PATH = f\"{DATASET_TRAIN_DIR_PATH}/superimposed_annotations\"\n",
        "SUPERIMPOSED_ANNOTATIONS_VALID_DIRECTORY_PATH = f\"{DATASET_VALID_DIR_PATH}/superimposed_annotations\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rename_videos(VIDEO_DIR_PATH):\n",
        "    for i, video_name in enumerate(os.listdir(VIDEO_DIR_PATH)):\n",
        "        video_path = f\"{VIDEO_DIR_PATH}/{video_name}\"\n",
        "        os.rename(video_path, f\"{VIDEO_DIR_PATH}/{i}.mp4\")\n",
        "\n",
        "# do the same for generic files with unknown extension\n",
        "def rename_files(path):\n",
        "    # TODO: Haven't tested this yet\n",
        "    for i, file_name in enumerate(os.listdir(path)):\n",
        "        file_path = f\"{path}/{file_name}\"\n",
        "        # get the extension of the file\n",
        "        file_extension = file_name.split(\".\")[-1]\n",
        "        # os.rename(file_path, f\"{path}/{i}.{file_extension}\")\n",
        "        print(file_path, f\"{path}/{i}.{file_extension}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAI8n81PO08E"
      },
      "source": [
        "### Convert videos into images\n",
        "\n",
        "The code below saves every `nth` frame from each video using the `FRAME_STRIDE` parameter. The code for this had to be placed in it's own Python file, [video_multiprocessing.py](/Users/eric/Desktop/2-Career/Projects/ObjectDetectionLL/video_multiprocessing.py), in order for the multiprocessing pool to work properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "FRAME_STRIDE = 1\n",
        "video_paths = sv.list_files_with_extensions(\n",
        "    directory=VIDEO_DIR_PATH,\n",
        "    extensions=[\"mov\", \"mp4\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15, 330)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assuming you have already split your video paths\n",
        "# TEST_VIDEO_PATHS, TRAIN_VIDEO_PATHS = video_paths[:2], video_paths[2:]\n",
        "TRAIN_VIDEO_PATHS = video_paths\n",
        "\n",
        "split_videos = False\n",
        "if split_videos:\n",
        "    # Now call the function to process videos in parallel\n",
        "    vm.process_videos_in_parallel(video_paths=TRAIN_VIDEO_PATHS, image_dir_path=IMAGE_DIR_PATH, frame_stride=FRAME_STRIDE)\n",
        "    # find the names of any files that may be missing for whatever reason\n",
        "    for i in range(0, len(os.listdir(VIDEO_DIR_PATH))):\n",
        "        if f\"{i}-00000.png\" not in os.listdir(IMAGE_DIR_PATH):\n",
        "            print(f\"{i}.png not found\")\n",
        "# count the number of images in the video and image directories\n",
        "len(os.listdir(VIDEO_DIR_PATH)) , len(os.listdir(IMAGE_DIR_PATH)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIVuc89kVp2w"
      },
      "source": [
        "## Annotate the dataset\n",
        "\n",
        "**TASK**: Create a labeled dataset from a directory of images using Meta's *Distillation with No Labels* (DINO) and *Segment Anything Model* (SAM)\n",
        "\n",
        "**this is not correct. SAM is not multimodal. MULTIMODAL**: SAM is a multimodal model, which means it can accept multiple types of input. In this case, SAM accepts both images and text captions as input. SAM will use its LLM capabilities to understand the text captions and segment the images.\n",
        "\n",
        "**WHY SAM?**: Before the release of multimodal models, computer vision experts would have to manully annotate thousands of images with special care for their deep learning models. This involved drawing boxes around pixels of interest, being careful to all of the pixels that include the object of interest while minimizing the amount of pixels in the background. Special considerations also needed to be made for objects of interest that appeared partially out of frame and for partially occluded objects of interest in the image. Each carefully drawn bounding polygon is then labeled from the set of classifications that you are hoping to train the target model to detect. This labor intensive process used to be a major obstacle for custom object detection models, and it is something I've had the pleasure of doing as recently as March of 2023, which was the release date of Meta's SAM. So to save time and moeny, we will use SAM as a Base Model to help create a labeled dataset.\n",
        "\n",
        "**ONTOLOGY**: An Ontology defines how your Base Model is prompted, what your Dataset will describe, and what your Target Model will predict. A simple Ontology is the CaptionOntology which prompts a Base Model with text captions and maps them to class names. Other Ontologies may, for instance, use a CLIP vector or example images instead of a text caption.\n",
        "\n",
        "**BASE MODEL**: A Base Model is a large foundation model that knows a lot about a lot. Base models are often multimodal and can perform many tasks. They're large, slow, and expensive. Examples of Base Models are Meta's SAM and OpenAI's GPT-4 multimodal variant.\n",
        "\n",
        "**LABELED DATASET**: The output from the Base Model is a labeled dataset, which is a set of auto-labeled images. First we'll review the data for mistakes; we can delete incorrectly labeled images from the dataset or we can use an annotation tool to edit the labels. Examples of annotation tools include: LabelMe, LabelImg, CVAT, SuperAnnotate, ScaleAI, Roboflow Annotator, and many more. \n",
        "\n",
        "**TARGET MODEL**: A Target Model is a model that is trained on a labeled dataset to perform a specific task. Target Models are often small, fast, cheap, and fine-tuned to perform a specific task very well, but they don't generalize well beyond the information described in their dataset. Examples of Target Models are YOLO and DETR. Once we have corrected all mistakes in our labeled dataset, then it is safe to proceed with training our Target Model - in this case YOLOv8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "24qFSVyhUV8C"
      },
      "outputs": [],
      "source": [
        "annotate = False\n",
        "if annotate:\n",
        "    labels_dict = {\n",
        "        \"The license plate on the motor vehicle\": \"license plate\", #0\n",
        "        # \"license plate\": \"license plate\", #0\n",
        "        # \"number plate\": \"license plate\", #1\n",
        "        # \"registration plate\": \"license plate\", #2\n",
        "        # \"license\": \"license plate\", #3\n",
        "        # \"license plate number\": \"license plate\", #4\n",
        "        # \"numberplate\": \"license plate\", #5\n",
        "    }\n",
        "    ontology=CaptionOntology(labels_dict)\n",
        "\n",
        "    base_model = GroundedSAM(ontology=ontology)\n",
        "    dataset = base_model.label(\n",
        "        input_folder=IMAGE_DIR_PATH,\n",
        "        extension=\".png\",\n",
        "        output_folder=DATASET_DIR_PATH)\n",
        "\n",
        "    len(dataset)\n",
        "else:\n",
        "    os.makedirs(ANNOTATIONS_TRAIN_DIRECTORY_PATH, exist_ok=True)\n",
        "    os.makedirs(ANNOTATIONS_VALID_DIRECTORY_PATH, exist_ok=True)\n",
        "    os.makedirs(IMAGES_TRAIN_DIRECTORY_PATH, exist_ok=True)\n",
        "    os.makedirs(IMAGES_VALID_DIRECTORY_PATH, exist_ok=True)\n",
        "#     with open(YOLO_DATA_YAML_PATH, \"w\") as f:\n",
        "#         sam_data_yaml = f.write('''names:\n",
        "# - license plate\n",
        "# nc: 1\n",
        "# train: /Users/eric/Desktop/2-Career/Projects/ObjectDetection/dataset/train/images\n",
        "# val: /Users/eric/Desktop/2-Career/Projects/ObjectDetection/dataset/valid/images\n",
        "# ''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize the ontologies that worked best\n",
        "\n",
        "Assuming more than one ontology was used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "if annotate:\n",
        "    labels_list = list(labels_dict.items())\n",
        "    labels_path = f'{DATASET_TRAIN_DIR_PATH}/labels'\n",
        "    for filename in os.listdir(labels_path):\n",
        "        with open(os.path.join(labels_path, filename), 'r') as f:\n",
        "            contents = f.readlines()\n",
        "            labels = []\n",
        "            for line in contents:\n",
        "                label_str = line.split(' ')[0]\n",
        "                mask = line[len(label_str):].strip().split(' ')\n",
        "                num_segmentation_points = len(mask) // 2\n",
        "                label_int = int(label_str)\n",
        "                labels += [(labels_list[label_int][0], labels_list[label_int][1], num_segmentation_points)]\n",
        "            if len(contents) > 0:\n",
        "                print(filename, len(contents), labels)\n",
        "\n",
        "    histogram = {}\n",
        "    for filename in os.listdir(labels_path):\n",
        "        with open(os.path.join(labels_path, filename), 'r') as f:\n",
        "            contents = f.readlines()\n",
        "            for line in contents:\n",
        "                label_str = line.split(' ')[0]\n",
        "                mask = line[len(label_str):].strip().split(' ')\n",
        "                num_segmentation_points = len(mask) // 2\n",
        "                label_int = int(label_str)\n",
        "                label_name = labels_list[label_int][0]\n",
        "                if label_name not in histogram:\n",
        "                    histogram[label_name] = 0\n",
        "                histogram[label_name] += 1\n",
        "\n",
        "    # plot this histogram\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.bar(range(len(histogram)), list(histogram.values()), align='center')\n",
        "    plt.xticks(range(len(histogram)), list(histogram.keys()))\n",
        "    plt.xlabel('Ontology key words')\n",
        "    plt.ylabel('Number of instances')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM9zH9BGZGp0"
      },
      "source": [
        "## Review the Annotations\n",
        "\n",
        "**TASK**: For the sake of time, we'll simply delete any incorrectly annotated images from the labeled dataset. This will take some manual effort, but much less effort than editing masks, which is something to consider if your dataset is small. \n",
        "\n",
        "**SUPERIMPOSE ANNOTATIONS FOR REVIEW**: The SAM model has placed all of the images in the `dataset/train/images` directory and the corresponding labels in the `dataset/train/labels` directory. Using Python, I'll superimpose the labels onto the image and save them to a new directory, `dataset/train/superimposed_annotations` so I can visually inspect the annotations. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_iou(mask1, mask2):\n",
        "    '''UNUSED: Calculates Intersection over Union (IoU) for two binary masks'''\n",
        "    intersection = np.logical_and(mask1, mask2)\n",
        "    union = np.logical_or(mask1, mask2)\n",
        "    iou = np.sum(intersection) / np.sum(union)\n",
        "    return iou\n",
        "\n",
        "def are_duplicates_based_on_overlap(mask1, mask2, threshold=0.95):\n",
        "    '''Calculates the mutual overlap for two binary masks and returns a boolean indicating whether they are duplicates'''\n",
        "    intersection = np.logical_and(mask1, mask2)\n",
        "    overlap_area_1 = np.sum(intersection) / np.sum(mask1)\n",
        "    overlap_area_2 = np.sum(intersection) / np.sum(mask2)\n",
        "    return overlap_area_1 >= threshold and overlap_area_2 >= threshold\n",
        "\n",
        "def parse_line(line):\n",
        "    '''Parses a line and returns the original label and coordinates'''\n",
        "    parts = line.split()\n",
        "    original_label = int(parts[0])\n",
        "    coordinates = np.array([float(coord) for coord in parts[1:]]).reshape(-1, 2)\n",
        "    return original_label, coordinates\n",
        "\n",
        "def create_mask(coordinates, image_dims):\n",
        "    # Scale the normalized coordinates to the image dimensions\n",
        "    scaled_coords = coordinates * np.array(image_dims)\n",
        "    # Ensure the coordinates are integers, as required by cv2.fillPoly\n",
        "    scaled_coords = np.round(scaled_coords).astype(np.int32)\n",
        "    # Create an empty black image and fill it with white in the polygon\n",
        "    mask = np.zeros(image_dims, dtype=np.uint8)\n",
        "    cv2.fillPoly(mask, [scaled_coords], color=1)\n",
        "    return mask\n",
        "\n",
        "# Read mask lines from the input file\n",
        "def consolidate_and_correct_masks(annotations_path, annotations_filtered_path):\n",
        "    # Define the new label groups based on original labels\n",
        "    label_groups = {\n",
        "        0: list(range(0, 1)),  # Labels 0-n will be consolidated into new label 0\n",
        "        # 1: list(range(1, 5)),  # Labels n+1-m will be consolidated into new label 1 for another label\n",
        "    }\n",
        "\n",
        "    # Reverse mapping for convenience: original label to new label\n",
        "    original_to_new_label = {original: new for new, originals in label_groups.items() for original in originals}\n",
        "    threshold = 0.95  # Threshold for considering masks as duplicates. # HACK: This is completely problem-specific.\n",
        "    image_dims = (2160, 3840)  # Dimensions of the images in the dataset # HACK: This won't fly with variable image dimensions. To improve, read the image dimensions from the image files themselves using cv2.imread.\n",
        "    for file in os.listdir(annotations_path):\n",
        "        if not file.endswith('.txt'):\n",
        "            continue\n",
        "        input_file_path = f'{annotations_path}/{file}'\n",
        "        with open(input_file_path, 'r') as f:\n",
        "            mask_lines = f.readlines()\n",
        "\n",
        "        unique_mask_lines = []\n",
        "\n",
        "        for line_i in mask_lines:\n",
        "            original_label_i, coords_i = parse_line(line_i)\n",
        "            new_label_i = original_to_new_label.get(original_label_i)\n",
        "            mask_i = create_mask(coords_i, image_dims)\n",
        "            is_duplicate = False\n",
        "            for line_j in unique_mask_lines:\n",
        "                new_label_j, coords_j = parse_line(line_j)\n",
        "                if new_label_j == new_label_i:  # Compare only within the same new label group\n",
        "                    mask_j = create_mask(coords_j, image_dims)\n",
        "                    if are_duplicates_based_on_overlap(mask_i, mask_j, threshold=threshold):\n",
        "                        is_duplicate = True\n",
        "                        break # break to avoid adding the duplicate mask to the unique mask lines\n",
        "            if not is_duplicate:\n",
        "                mask_area = np.sum(mask_i)\n",
        "                if mask_area < 2000: # HACK: This is completely problem-specific. In this case, we're looking for license plates, and after investigating, I found that their mask area is typically well below 10000.\n",
        "                    unique_mask_lines.append(f\"{new_label_i} {' '.join(map(str, coords_i.flatten()))}\")\n",
        "\n",
        "        # Write the consolidated mask data with new labels to the output file\n",
        "        output_file_path = f'{annotations_filtered_path}/{file}'\n",
        "        with open(output_file_path, 'w') as f:\n",
        "            for i, line in enumerate(unique_mask_lines):\n",
        "                if i == len(unique_mask_lines) - 1:\n",
        "                    f.write(line)\n",
        "                else:\n",
        "                    f.write(f'{line}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "superimpose = False\n",
        "if superimpose:\n",
        "    IMAGE_DIR = 'dataset/train/images'\n",
        "    BBOX_DIR = 'dataset/train/labels'\n",
        "    OUTPUT_DIR = 'dataset/train/superimposed_annotations'\n",
        "    sb.superimpose_boxes(IMAGE_DIR, BBOX_DIR, OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ›‘**MANUAL VISUAL INSPECTION**ðŸ›‘: To review the annotations, I'll go into Finder on my Mac, preview the files one-by-one with the spacebar and arrow keys, and simply delete the image file that have mistakes. Don't skip this step! The quality of your dataset will determine the quality of your model; tedious as it may be, it's worth the effort to ensure your dataset is clean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Great! Let's move on.\n"
          ]
        }
      ],
      "source": [
        "# inspection_complete = input(\"Have you inspected the annotations? (y/n): \")\n",
        "inspection_complete = 'y'\n",
        "if inspection_complete == \"y\":\n",
        "    print(\"Great! Let's move on.\")\n",
        "else:\n",
        "    print(\"Please inspect the annotations before continuing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**REMOVE MISTAKES**: Now that we've decided which image files to remove, we must reflect these changes in the folders that are actually used to train YOLO. We'll use Python to programatically identify those filenames that need to be removed from both the `dataset/train/images` and `dataset/train/labels` directories. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "if annotate:\n",
        "    for image_path, annotations_path, _, superimposed_annotations_path in [train_paths, valid_paths]:\n",
        "        superimposed_annotation_file_names = os.listdir(superimposed_annotations_path)\n",
        "        image_file_names = os.listdir(image_path)\n",
        "        # find the names of any files that do not exist in the superimposed annotations directory, but do exist in the images directory, and delete them from both the images and labels directories.\n",
        "        superimposed_annotation_file_names_copy = superimposed_annotation_file_names.copy()\n",
        "        count = 0\n",
        "        deleted = []\n",
        "        for image_file_name in image_file_names:\n",
        "            # print(f\"{image_file_name}\")\n",
        "            if f\"{image_file_name.split('.')[0]}.jpg\" in superimposed_annotation_file_names_copy:\n",
        "                # print(image_file_name)\n",
        "                superimposed_annotation_file_names_copy.remove(f\"{image_file_name.split('.')[0]}.jpg\")\n",
        "                continue\n",
        "            else:\n",
        "                count += 1\n",
        "                os.remove(f\"{image_path}/{image_file_name}\") \n",
        "                os.remove(f\"{annotations_path}/{image_file_name[:-4]}.txt\")\n",
        "                deleted.append(image_file_name.split('.')[0])\n",
        "        print(f\"{count} images deleted from {image_path} and their corresponding label files from {annotations_path}.\") \n",
        "        print(f'{len(os.listdir(image_path))} images remain in {image_path} and {len(os.listdir(annotations_path))} label files remain in {annotations_path}.')\n",
        "        print(f'Deleted Files: {deleted}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ9ESdTCagkm"
      },
      "source": [
        "## Train the target model\n",
        "\n",
        "**START TRAINING**: Finally, once the annotations are pristine, we can proceed with training our Target Model, YOLOv8. We'll take the pre-trained weights from the YOLOv8 model, [yolov8m.pt](/Users/eric/Desktop/2-Career/Projects/ObjectDetectionLL/yolov8m.pt), and fine-tune them on our labeled dataset. The training session will save the weights to a couple new files, [runs/detect/train/weights/best.pt](/Users/eric/Desktop/2-Career/Projects/ObjectDetectionLL/runs/detect/train/weights/best.pt), which is the best weights file, and [runs/detect/train/weights/last.pt](/Users/eric/Desktop/2-Career/Projects/ObjectDetectionLL/runs/detect/train/weights/last.pt), which is the most recent weights file. \n",
        "\n",
        "We'll use the [best.pt](/Users/eric/Desktop/2-Career/Projects/ObjectDetectionLL/runs/detect/train/weights/best.pt) weights file to demonstrate its inference capabilities on our test set, and we can use the [last.pt](/Users/eric/Desktop/2-Career/Projects/ObjectDetectionLL/runs/detect/train/weights/last.pt) weights file to resume training in place of yolov8m.pt if we want to make it more robust with additional epochs and/or data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of True Negatives (Backgrounds without LPs) in train set: 0.00% of 295 images\n",
            "Percentage of True Negatives (Backgrounds without LPs) in valid set: 0.00% of 95 images\n"
          ]
        }
      ],
      "source": [
        "# See the proportion of images with no annotations (True Negatives)\n",
        "for folder in ['train', 'valid']:\n",
        "    image_dir = f'{project}/dataset/{folder}/images'\n",
        "    image_filenames = os.listdir(image_dir)\n",
        "    label_dir = f'{project}/dataset/{folder}/labels'\n",
        "    label_filenames = os.listdir(label_dir)\n",
        "    num_empty = 0\n",
        "    for label_filename in label_filenames:\n",
        "        with open(os.path.join(label_dir, label_filename), 'r') as file:\n",
        "            contents = file.read()\n",
        "            if not contents:\n",
        "                num_empty += 1\n",
        "    print(f'Percentage of True Negatives (Backgrounds without LPs) in {folder} set: {num_empty / len(label_filenames) * 100:.2f}% of {len(label_filenames)} images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9sLzt1MdalB9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "New https://pypi.org/project/ultralytics/8.0.230 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
            "'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
            "Ultralytics YOLOv8.0.81 ðŸš€ Python-3.10.12 torch-2.1.0 MPS\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=/Users/eric/Desktop/2-Career/Projects/ObjectDetection/runs/detect/train_dogpark_2023.12.26_17.08.07_n_im640_ep100_baNone_seNone/weights/last.pt, data=/Users/eric/Desktop/2-Career/Projects/ObjectDetection/dog_park/dataset/data.yaml, epochs=100, patience=50, batch=16, imgsz=1088, save=True, save_period=-1, cache=False, device=mps, workers=8, project=None, name=train_dog_park_2023.12.27_11.39.50_n_im1088_ep100_baNone_seNone, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train_dog_park_2023.12.27_11.39.50_n_im1088_ep100_baNone_seNone\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.Detect                [1, [64, 128, 256]]           \n",
            "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
            "Model summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train_dog_park_2023.12.27_11.39.50_n_im1088_ep100_baNone_seNone', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/eric/Desktop/2-Career/Projects/ObjectDetection/dog_park/dataset/train/labels... 295 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [00:02<00:00, 103.97it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/eric/Desktop/2-Career/Projects/ObjectDetection/dog_park/dataset/train/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/eric/Desktop/2-Career/Projects/ObjectDetection/dog_park/dataset/valid/labels... 95 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:01<00:00, 63.48it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/eric/Desktop/2-Career/Projects/ObjectDetection/dog_park/dataset/valid/labels.cache\n",
            "Plotting labels to runs/detect/train_dog_park_2023.12.27_11.39.50_n_im1088_ep100_baNone_seNone/labels.jpg... \n",
            "Image sizes 1088 train, 1088 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train_dog_park_2023.12.27_11.39.50_n_im1088_ep100_baNone_seNone\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      1/100         0G      1.397      1.231     0.9058         33       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:42<00:00,  2.22s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:36<00:00, 12.11s/it]\n",
            "                   all         95        285      0.543      0.488      0.407      0.199\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      2/100         0G        1.3     0.9967     0.8752         28       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:42<00:00,  2.24s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:23<00:00,  7.74s/it]\n",
            "                   all         95        285      0.524       0.53      0.369      0.179\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      3/100         0G      1.278          1     0.8571         41       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:48<00:00,  2.54s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:28<00:00,  9.47s/it]\n",
            "                   all         95        285      0.543      0.523      0.436      0.218\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      4/100         0G      1.311      0.961     0.8603         34       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:44<00:00,  2.36s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  3.78s/it]\n",
            "                   all         95        285      0.529      0.505      0.441      0.221\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      5/100         0G      1.317      1.029     0.8601         21       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:48<00:00,  2.53s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.23s/it]\n",
            "                   all         95        285      0.523      0.522      0.438      0.224\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      6/100         0G      1.373      1.023     0.8624         53       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:53<00:00,  2.80s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.03s/it]\n",
            "                   all         95        285      0.496      0.505      0.385      0.177\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      7/100         0G      1.286       1.11     0.8649         40       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:56<00:00,  3.00s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  3.87s/it]\n",
            "                   all         95        285      0.507      0.537      0.434      0.177\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      8/100         0G      1.332     0.9884     0.8682         43       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:43<00:00,  2.27s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.90s/it]\n",
            "                   all         95        285      0.511      0.539      0.407      0.177\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      9/100         0G      1.299       1.01     0.8551         44       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:52<00:00,  2.75s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:19<00:00,  6.63s/it]\n",
            "                   all         95        285       0.49      0.502      0.342       0.16\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     10/100         0G      1.314     0.9295     0.8587         43       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:45<00:00,  2.40s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:21<00:00,  7.30s/it]\n",
            "                   all         95        285      0.511      0.526      0.382      0.164\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     11/100         0G      1.323     0.9949     0.8573         47       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:54<00:00,  2.84s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.03s/it]\n",
            "                   all         95        285      0.541      0.537      0.405      0.205\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     12/100         0G      1.268     0.9303     0.8567         38       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:42<00:00,  2.26s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.96s/it]\n",
            "                   all         95        285        0.5      0.515      0.406      0.198\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     13/100         0G      1.256     0.9316     0.8514         43       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:46<00:00,  2.44s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.97s/it]\n",
            "                   all         95        285      0.515      0.523      0.397      0.199\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     14/100         0G      1.271     0.9366     0.8496         34       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:44<00:00,  2.34s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.94s/it]\n",
            "                   all         95        285      0.504      0.544      0.388      0.178\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     15/100         0G      1.248     0.8956     0.8434         30       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:40<00:00,  2.14s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.12s/it]\n",
            "                   all         95        285      0.544      0.544      0.394      0.204\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     16/100         0G      1.225      0.913     0.8435         48       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:41<00:00,  2.16s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.81s/it]\n",
            "                   all         95        285      0.508      0.502      0.327      0.158\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     17/100         0G      1.237     0.8837     0.8368         36       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:43<00:00,  2.30s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.88s/it]\n",
            "                   all         95        285      0.512      0.512      0.353      0.169\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     18/100         0G      1.228     0.8946     0.8419         42       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:31<00:00,  1.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.64s/it]\n",
            "                   all         95        285       0.53      0.512      0.407      0.198\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     19/100         0G      1.186     0.9114     0.8446         49       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:38<00:00,  2.01s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.93s/it]\n",
            "                   all         95        285      0.505      0.526       0.38      0.194\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     20/100         0G      1.168     0.9065     0.8344         36       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:29<00:00,  1.54s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.83s/it]\n",
            "                   all         95        285       0.53       0.53      0.368        0.2\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     21/100         0G      1.199     0.8902     0.8309         53       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:35<00:00,  1.88s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.89s/it]\n",
            "                   all         95        285      0.512       0.54      0.387      0.199\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     22/100         0G      1.194     0.8608      0.832         40       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:49<00:00,  2.59s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.85s/it]\n",
            "                   all         95        285      0.534       0.54        0.4      0.201\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     23/100         0G      1.171      0.854     0.8311         32       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:32<00:00,  1.73s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:24<00:00,  8.31s/it]\n",
            "                   all         95        285      0.496      0.519      0.405      0.214\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     24/100         0G      1.191     0.8422     0.8297         34       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:43<00:00,  2.31s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.28s/it]\n",
            "                   all         95        285      0.527      0.533       0.44      0.236\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     25/100         0G      1.146     0.8344     0.8314         19       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:37<00:00,  1.95s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.04s/it]\n",
            "                   all         95        285      0.513      0.523      0.408      0.223\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     26/100         0G       1.11     0.8631     0.8262         24       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:49<00:00,  2.61s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.95s/it]\n",
            "                   all         95        285      0.508      0.533      0.401      0.209\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     27/100         0G      1.146     0.8432     0.8276         37       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:35<00:00,  1.85s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.01s/it]\n",
            "                   all         95        285      0.502      0.519      0.419      0.222\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     28/100         0G      1.127     0.8317      0.827         26       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:35<00:00,  1.86s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.58s/it]\n",
            "                   all         95        285      0.557      0.556       0.44      0.265\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     29/100         0G      1.127     0.8376     0.8341         37       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:25<00:00,  1.36s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.90s/it]\n",
            "                   all         95        285      0.496      0.495      0.356       0.19\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     30/100         0G      1.094     0.8487     0.8233         51       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:36<00:00,  1.90s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.85s/it]\n",
            "                   all         95        285      0.502       0.56      0.415      0.237\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     31/100         0G       1.12      0.806     0.8284         38       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.52s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.65s/it]\n",
            "                   all         95        285       0.52      0.558      0.451      0.257\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     32/100         0G      1.097     0.8416     0.8239         38       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:30<00:00,  1.63s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.87s/it]\n",
            "                   all         95        285      0.517      0.554      0.392      0.218\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     33/100         0G      1.078     0.8206     0.8334         46       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:29<00:00,  1.53s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.93s/it]\n",
            "                   all         95        285      0.501      0.547      0.382      0.221\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     34/100         0G      1.075       0.85     0.8218         42       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:21<00:00,  1.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.90s/it]\n",
            "                   all         95        285      0.487      0.551      0.408      0.239\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     35/100         0G      1.118     0.8378     0.8256         43       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:32<00:00,  1.70s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.89s/it]\n",
            "                   all         95        285      0.496      0.538      0.378        0.2\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     36/100         0G       1.06     0.8246     0.8164         34       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:46<00:00,  2.43s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.90s/it]\n",
            "                   all         95        285      0.498      0.526      0.382      0.238\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     37/100         0G      1.069     0.8228     0.8265         33       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:32<00:00,  1.71s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.85s/it]\n",
            "                   all         95        285      0.531       0.53      0.404      0.236\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     38/100         0G      1.072     0.8257     0.8223         27       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:37<00:00,  1.96s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.92s/it]\n",
            "                   all         95        285      0.527      0.554      0.409      0.251\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     39/100         0G      1.038     0.8182     0.8187         51       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:30<00:00,  1.63s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.86s/it]\n",
            "                   all         95        285      0.545      0.561      0.434      0.257\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     40/100         0G      1.079     0.8216     0.8166         22       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:36<00:00,  1.90s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.08s/it]\n",
            "                   all         95        285      0.493      0.509      0.344      0.199\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     41/100         0G      1.052     0.8096      0.813         55       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:30<00:00,  1.62s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.87s/it]\n",
            "                   all         95        285      0.488      0.516      0.334      0.189\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     42/100         0G      1.049     0.8127     0.8175         33       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:24<00:00,  1.28s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.93s/it]\n",
            "                   all         95        285      0.491      0.521      0.337      0.198\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     43/100         0G      1.025     0.8015     0.8178         58       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:21<00:00,  1.14s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.46s/it]\n",
            "                   all         95        285      0.496      0.505      0.347      0.204\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     44/100         0G     0.9942      0.818     0.8137         24       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:21<00:00,  1.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.81s/it]\n",
            "                   all         95        285      0.544      0.589      0.467      0.295\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     45/100         0G      1.024     0.7969     0.8109         36       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:33<00:00,  1.75s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.53s/it]\n",
            "                   all         95        285      0.571      0.607      0.493      0.325\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     46/100         0G      1.008     0.8057     0.8186         34       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:26<00:00,  1.39s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.79s/it]\n",
            "                   all         95        285      0.513      0.565      0.419      0.263\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     47/100         0G      1.038     0.7986     0.8156         36       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:23<00:00,  1.22s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.38s/it]\n",
            "                   all         95        285       0.52      0.558      0.411      0.263\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     48/100         0G      1.012     0.8119     0.8134         49       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:26<00:00,  1.38s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.44s/it]\n",
            "                   all         95        285      0.491      0.528      0.358      0.215\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     49/100         0G     0.9896     0.7971     0.8163         28       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:19<00:00,  1.01s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.82s/it]\n",
            "                   all         95        285      0.494      0.558      0.385      0.243\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     50/100         0G     0.9743     0.8053     0.8153         54       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.52s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.82s/it]\n",
            "                   all         95        285      0.539       0.54      0.411       0.26\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     51/100         0G     0.9942     0.8069     0.8162         26       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:19<00:00,  1.02s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.78s/it]\n",
            "                   all         95        285      0.533      0.552      0.431      0.289\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     52/100         0G     0.9893     0.7888     0.8132         49       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.52s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.80s/it]\n",
            "                   all         95        285       0.52      0.532      0.429       0.29\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     53/100         0G     0.9723     0.7929     0.8085         39       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:24<00:00,  1.32s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.80s/it]\n",
            "                   all         95        285      0.539      0.558      0.446      0.299\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     54/100         0G      0.951     0.7877     0.8033         38       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:29<00:00,  1.54s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.80s/it]\n",
            "                   all         95        285      0.538      0.543      0.416       0.25\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     55/100         0G     0.9806     0.7767     0.8173         40       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.51s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.82s/it]\n",
            "                   all         95        285      0.558      0.561      0.457      0.295\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     56/100         0G     0.9605     0.7893     0.8123         28       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:24<00:00,  1.27s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.82s/it]\n",
            "                   all         95        285      0.559       0.56      0.464      0.303\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     57/100         0G     0.9505     0.7926     0.8076         34       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:23<00:00,  1.22s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.76s/it]\n",
            "                   all         95        285      0.577      0.596      0.518      0.351\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     58/100         0G     0.9593     0.7936     0.8117         35       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:35<00:00,  1.85s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.82s/it]\n",
            "                   all         95        285      0.525      0.596      0.478      0.323\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     59/100         0G     0.9429     0.7688     0.8041         32       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:18<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.82s/it]\n",
            "                   all         95        285      0.567      0.586      0.502      0.339\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     60/100         0G     0.9528     0.7746     0.8069         39       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:24<00:00,  1.29s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.81s/it]\n",
            "                   all         95        285      0.566      0.614      0.503      0.344\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     61/100         0G     0.9429     0.7844     0.8046         44       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:22<00:00,  1.16s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.84s/it]\n",
            "                   all         95        285      0.562      0.596      0.468      0.319\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     62/100         0G     0.9305     0.7749     0.8014         25       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:36<00:00,  1.93s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.51s/it]\n",
            "                   all         95        285      0.538      0.551      0.401       0.27\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     63/100         0G     0.9219     0.7926     0.8116         25       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:24<00:00,  1.27s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.85s/it]\n",
            "                   all         95        285      0.561      0.561      0.453      0.314\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     64/100         0G     0.9246     0.7619     0.8016         32       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:22<00:00,  1.18s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.84s/it]\n",
            "                   all         95        285        0.6      0.575      0.558       0.41\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     65/100         0G     0.9011     0.7751     0.8032         37       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:26<00:00,  1.41s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.85s/it]\n",
            "                   all         95        285       0.59      0.604      0.533      0.372\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     66/100         0G     0.9209     0.7822     0.8101         33       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:35<00:00,  1.85s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.82s/it]\n",
            "                   all         95        285      0.584        0.6      0.525      0.373\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     67/100         0G     0.9148     0.7812     0.8008         34       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:24<00:00,  1.30s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.84s/it]\n",
            "                   all         95        285      0.559        0.6      0.472       0.32\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     68/100         0G     0.8988     0.7721     0.8068         26       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:31<00:00,  1.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.80s/it]\n",
            "                   all         95        285      0.565      0.582      0.504      0.342\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     69/100         0G     0.9004     0.7892     0.8123         18       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:31<00:00,  1.64s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.83s/it]\n",
            "                   all         95        285      0.581      0.613      0.547       0.38\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     70/100         0G     0.9034     0.7695     0.8082         29       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:19<00:00,  1.03s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.85s/it]\n",
            "                   all         95        285      0.566      0.607      0.486      0.338\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     71/100         0G     0.8901     0.7769     0.8103         49       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:26<00:00,  1.42s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.85s/it]\n",
            "                   all         95        285      0.534      0.547      0.413      0.286\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     72/100         0G     0.8816     0.7759     0.8018         48       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:22<00:00,  1.16s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.81s/it]\n",
            "                   all         95        285      0.516      0.526      0.336      0.218\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     73/100         0G     0.8759     0.7831     0.8029         33       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:22<00:00,  1.18s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.89s/it]\n",
            "                   all         95        285      0.547      0.565      0.419      0.282\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     74/100         0G      0.881     0.7685     0.7994         38       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:20<00:00,  1.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.88s/it]\n",
            "                   all         95        285      0.565      0.593      0.493      0.353\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     75/100         0G     0.8661     0.7606      0.797         19       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:35<00:00,  1.85s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.84s/it]\n",
            "                   all         95        285      0.558      0.563      0.448      0.317\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     76/100         0G     0.8783     0.7532     0.7984         37       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:22<00:00,  1.18s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.87s/it]\n",
            "                   all         95        285      0.561      0.582       0.45      0.323\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     77/100         0G     0.8661     0.7855     0.8006         48       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:22<00:00,  1.18s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.86s/it]\n",
            "                   all         95        285      0.576      0.582      0.472      0.348\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     78/100         0G     0.8435     0.7661     0.8014         33       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:25<00:00,  1.32s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.84s/it]\n",
            "                   all         95        285      0.566      0.607      0.489      0.354\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     79/100         0G     0.8763     0.7667     0.8025         35       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:24<00:00,  1.28s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.81s/it]\n",
            "                   all         95        285       0.57      0.579      0.486      0.354\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     80/100         0G     0.8686     0.7799     0.8044         35       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:19<00:00,  1.02s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.85s/it]\n",
            "                   all         95        285       0.58       0.61      0.517      0.376\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     81/100         0G     0.8503     0.7518     0.8017         46       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:30<00:00,  1.58s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.80s/it]\n",
            "                   all         95        285      0.547      0.582      0.484      0.346\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     82/100         0G     0.8383     0.7728      0.791         43       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:17<00:00,  1.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.79s/it]\n",
            "                   all         95        285      0.545      0.558      0.445      0.318\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     83/100         0G     0.8485     0.7567     0.8014         39       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:19<00:00,  1.03s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.81s/it]\n",
            "                   all         95        285      0.601      0.607      0.525      0.374\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     84/100         0G       0.81      0.764     0.7941         46       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:17<00:00,  1.07it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.85s/it]\n",
            "                   all         95        285      0.612      0.635      0.611      0.432\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     85/100         0G     0.8717     0.7645     0.7996         39       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:24<00:00,  1.28s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.85s/it]\n",
            "                   all         95        285      0.585      0.609      0.517      0.375\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     86/100         0G      0.831     0.7498     0.7988         33       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:17<00:00,  1.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.92s/it]\n",
            "                   all         95        285      0.557      0.604      0.489      0.363\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     87/100         0G      0.815     0.7581     0.7962         41       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.48s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.80s/it]\n",
            "                   all         95        285      0.542      0.582      0.456      0.331\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     88/100         0G     0.8203     0.7503     0.7972         39       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:24<00:00,  1.28s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.87s/it]\n",
            "                   all         95        285      0.531      0.572      0.436      0.313\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     89/100         0G     0.8027     0.7519     0.7948         37       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:26<00:00,  1.42s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.86s/it]\n",
            "                   all         95        285      0.559      0.604      0.495      0.361\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     90/100         0G     0.8197      0.772     0.7923         46       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:19<00:00,  1.03s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.86s/it]\n",
            "                   all         95        285      0.562      0.593       0.49       0.36\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     91/100         0G     0.8179     0.7549     0.7918         35       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:27<00:00,  1.44s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.84s/it]\n",
            "                   all         95        285      0.568      0.607      0.519      0.387\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     92/100         0G       0.81     0.7557     0.7904         55       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:34<00:00,  1.82s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.85s/it]\n",
            "                   all         95        285      0.557      0.607      0.489      0.362\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     93/100         0G     0.8074       0.76     0.7943         32       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:26<00:00,  1.41s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.84s/it]\n",
            "                   all         95        285      0.569      0.604      0.489      0.369\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     94/100         0G     0.7617     0.7533     0.7848         44       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:25<00:00,  1.33s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.81s/it]\n",
            "                   all         95        285      0.557        0.6       0.48      0.352\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     95/100         0G     0.7997     0.7286     0.7891         40       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:22<00:00,  1.18s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.85s/it]\n",
            "                   all         95        285      0.563      0.611      0.466      0.343\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     96/100         0G      0.791     0.7576     0.7968         41       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:30<00:00,  1.62s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.86s/it]\n",
            "                   all         95        285      0.562      0.611      0.484      0.359\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     97/100         0G     0.7861     0.7464     0.7964         29       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:22<00:00,  1.19s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.88s/it]\n",
            "                   all         95        285      0.596      0.602      0.515      0.386\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     98/100         0G      0.803     0.7588     0.7934         52       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:25<00:00,  1.32s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.85s/it]\n",
            "                   all         95        285      0.578      0.582      0.505      0.383\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     99/100         0G     0.7908     0.7412     0.7874         37       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:23<00:00,  1.24s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.90s/it]\n",
            "                   all         95        285      0.565      0.621      0.537      0.399\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    100/100         0G     0.7655     0.7449     0.7912         33       1088: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:20<00:00,  1.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.16s/it]\n",
            "                   all         95        285      0.561      0.607      0.516      0.385\n",
            "\n",
            "100 epochs completed in 1.099 hours.\n",
            "Optimizer stripped from runs/detect/train_dog_park_2023.12.27_11.39.50_n_im1088_ep100_baNone_seNone/weights/last.pt, 6.3MB\n",
            "Optimizer stripped from runs/detect/train_dog_park_2023.12.27_11.39.50_n_im1088_ep100_baNone_seNone/weights/best.pt, 6.3MB\n",
            "\n",
            "Validating runs/detect/train_dog_park_2023.12.27_11.39.50_n_im1088_ep100_baNone_seNone/weights/best.pt...\n",
            "'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
            "Ultralytics YOLOv8.0.81 ðŸš€ Python-3.10.12 torch-2.1.0 MPS\n",
            "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.59s/it]\n",
            "                   all         95        285      0.609      0.632      0.609      0.433\n",
            "Speed: 3.3ms preprocess, 106.0ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train_dog_park_2023.12.27_11.39.50_n_im1088_ep100_baNone_seNone\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "datetime_now = datetime.now().strftime(\"%Y.%m.%d_%H.%M.%S\")\n",
        "start_from_scratch = False\n",
        "\n",
        "model_size = 'n'\n",
        "epochs = 100\n",
        "device = 'mps'\n",
        "imgsz = 1080 # Check images in letterbox experiment to see how small we can go before the license plates become unrecognizable.\n",
        "imgsz = int(32 * round(imgsz / 32)) # ensure imgsz is a multiple of 32\n",
        "seed = None # set the seed for reproducibility\n",
        "batch = None # higher increases training speed, but may reduce mAP accuracy. Yann LeCun recommends <=32.\n",
        "save_dir = f'train_{project}_{datetime_now}_{model_size}_im{imgsz}_ep{epochs}_ba{batch}_se{seed}'\n",
        "\n",
        "yml = YOLO_DATA_YAML_PATH\n",
        "if start_from_scratch:\n",
        "    target_model = YOLO(f\"yolov8{model_size}.pt\")\n",
        "else:\n",
        "    if project == 'dog_park':\n",
        "        latest_train_dir = 'train_dogpark_2023.12.26_17.08.07_n_im640_ep100_baNone_seNone'\n",
        "    elif project == 'license_plates':\n",
        "        latest_train_dir = 'train_LP_2023.12.26_14.22.25_n_im640_ep20_baNone_seNone'\n",
        "    elif project == 'hockey':\n",
        "        latest_train_dir = 'train_hockey_2023.12.06_14.04.17_n_im640_ep1_baNone_seNone'\n",
        "    else:\n",
        "        latest_train_dir = input(\"Enter the name of the latest training directory: \")\n",
        "\n",
        "    target_model = YOLO(f'{HOME}/runs/detect/{latest_train_dir}/weights/last.pt')\n",
        "\n",
        "if seed is None:\n",
        "    # target_model.train(data=yml, epochs=epochs, device=device, imgsz=imgsz, batch=batch, name=save_dir)\n",
        "    target_model.train(data=yml, epochs=epochs, device=device, imgsz=imgsz, name=save_dir)\n",
        "else:\n",
        "    target_model.train(data=yml, epochs=epochs, device=device, imgsz=imgsz, seed=seed, batch=batch, name=save_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: 'dog_park_0_2023.12.26_12.50.03_n_im640_ep20_baNone_seNone'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m train_folder:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m----> 5\u001b[0m         number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m number \u001b[38;5;241m>\u001b[39m max_number:\n\u001b[1;32m      7\u001b[0m             max_number \u001b[38;5;241m=\u001b[39m number\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'dog_park_0_2023.12.26_12.50.03_n_im640_ep20_baNone_seNone'"
          ]
        }
      ],
      "source": [
        "train_folder = sorted(os.listdir(f\"{HOME}/runs/detect/\"))\n",
        "max_number = 0\n",
        "for file in train_folder:\n",
        "    if 'train' in file:\n",
        "        number = int(file.replace('train', '0'))\n",
        "        if number > max_number:\n",
        "            max_number = number\n",
        "if max_number == 0:\n",
        "    max_number = ''\n",
        "latest_train_dir = f'train{max_number}'\n",
        "latest_train_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlv5QA3Vg06Z",
        "outputId": "1767c48d-64b8-4da3-e1f4-ef1a7d68bf8f"
      },
      "outputs": [],
      "source": [
        "!ls {HOME}/runs/detect/{latest_train_dir}/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Tn0fgNgoQl"
      },
      "source": [
        "## Evaluate target model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "z8EW50NAgSdU",
        "outputId": "336bf84d-1e38-49de-d001-3de817897078"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/Users/eric/Desktop/2-Career/Projects/ObjectDetection/runs/detect/train_2023.12.05_17.00.11_n_im640_ep64_baNone_seNone/confusion_matrix.png'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/eric/Desktop/2-Career/Projects/ObjectDetection/object_detection_pipeline.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/eric/Desktop/2-Career/Projects/ObjectDetection/object_detection_pipeline.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Image(filename\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mHOME\u001b[39m}\u001b[39;49;00m\u001b[39m/runs/detect/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlatest_train_dir\u001b[39m}\u001b[39;49;00m\u001b[39m/confusion_matrix.png\u001b[39;49m\u001b[39m'\u001b[39;49m, width\u001b[39m=\u001b[39;49m\u001b[39m600\u001b[39;49m)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/CV/lib/python3.10/site-packages/IPython/core/display.py:970\u001b[0m, in \u001b[0;36mImage.__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munconfined \u001b[39m=\u001b[39m unconfined\n\u001b[1;32m    969\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malt \u001b[39m=\u001b[39m alt\n\u001b[0;32m--> 970\u001b[0m \u001b[39msuper\u001b[39;49m(Image, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data\u001b[39m=\u001b[39;49mdata, url\u001b[39m=\u001b[39;49murl, filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m    971\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata)\n\u001b[1;32m    973\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m, {}):\n\u001b[1;32m    974\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m=\u001b[39m metadata[\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m]\n",
            "File \u001b[0;32m/opt/anaconda3/envs/CV/lib/python3.10/site-packages/IPython/core/display.py:327\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 327\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreload()\n\u001b[1;32m    328\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_data()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/CV/lib/python3.10/site-packages/IPython/core/display.py:1005\u001b[0m, in \u001b[0;36mImage.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[39msuper\u001b[39;49m(Image,\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mreload()\n\u001b[1;32m   1006\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretina:\n\u001b[1;32m   1007\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retina_shape()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/CV/lib/python3.10/site-packages/IPython/core/display.py:353\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     encoding \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_flags \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_flags, encoding\u001b[39m=\u001b[39;49mencoding) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    354\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m    355\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[39m# Deferred import\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/eric/Desktop/2-Career/Projects/ObjectDetection/runs/detect/train_2023.12.05_17.00.11_n_im640_ep64_baNone_seNone/confusion_matrix.png'"
          ]
        }
      ],
      "source": [
        "Image(filename=f'{HOME}/runs/detect/{latest_train_dir}/confusion_matrix.png', width=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/Users/eric/Desktop/2-Career/Projects/ObjectDetection/runs/detect/train_2023.12.05_17.00.11_n_im640_ep64_baNone_seNone/results.png'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/eric/Desktop/2-Career/Projects/ObjectDetection/object_detection_pipeline.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/eric/Desktop/2-Career/Projects/ObjectDetection/object_detection_pipeline.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Image(filename\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mHOME\u001b[39m}\u001b[39;49;00m\u001b[39m/runs/detect/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlatest_train_dir\u001b[39m}\u001b[39;49;00m\u001b[39m/results.png\u001b[39;49m\u001b[39m'\u001b[39;49m, width\u001b[39m=\u001b[39;49m\u001b[39m600\u001b[39;49m)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/CV/lib/python3.10/site-packages/IPython/core/display.py:970\u001b[0m, in \u001b[0;36mImage.__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munconfined \u001b[39m=\u001b[39m unconfined\n\u001b[1;32m    969\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malt \u001b[39m=\u001b[39m alt\n\u001b[0;32m--> 970\u001b[0m \u001b[39msuper\u001b[39;49m(Image, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data\u001b[39m=\u001b[39;49mdata, url\u001b[39m=\u001b[39;49murl, filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m    971\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata)\n\u001b[1;32m    973\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m, {}):\n\u001b[1;32m    974\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m=\u001b[39m metadata[\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m]\n",
            "File \u001b[0;32m/opt/anaconda3/envs/CV/lib/python3.10/site-packages/IPython/core/display.py:327\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 327\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreload()\n\u001b[1;32m    328\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_data()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/CV/lib/python3.10/site-packages/IPython/core/display.py:1005\u001b[0m, in \u001b[0;36mImage.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[39msuper\u001b[39;49m(Image,\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mreload()\n\u001b[1;32m   1006\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretina:\n\u001b[1;32m   1007\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retina_shape()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/CV/lib/python3.10/site-packages/IPython/core/display.py:353\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     encoding \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_flags \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_flags, encoding\u001b[39m=\u001b[39;49mencoding) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    354\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m    355\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[39m# Deferred import\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/eric/Desktop/2-Career/Projects/ObjectDetection/runs/detect/train_2023.12.05_17.00.11_n_im640_ep64_baNone_seNone/results.png'"
          ]
        }
      ],
      "source": [
        "Image(filename=f'{HOME}/runs/detect/{latest_train_dir}/results.png', width=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/Users/eric/Desktop/2-Career/Projects/ObjectDetection/runs/detect/train_2023.12.05_17.00.11_n_im640_ep64_baNone_seNone/val_batch0_pred.jpg'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/eric/Desktop/2-Career/Projects/ObjectDetection/object_detection_pipeline.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/eric/Desktop/2-Career/Projects/ObjectDetection/object_detection_pipeline.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Image(filename\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mHOME\u001b[39m}\u001b[39;49;00m\u001b[39m/runs/detect/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlatest_train_dir\u001b[39m}\u001b[39;49;00m\u001b[39m/val_batch0_pred.jpg\u001b[39;49m\u001b[39m'\u001b[39;49m, width\u001b[39m=\u001b[39;49m\u001b[39m600\u001b[39;49m)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/CV/lib/python3.10/site-packages/IPython/core/display.py:970\u001b[0m, in \u001b[0;36mImage.__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munconfined \u001b[39m=\u001b[39m unconfined\n\u001b[1;32m    969\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malt \u001b[39m=\u001b[39m alt\n\u001b[0;32m--> 970\u001b[0m \u001b[39msuper\u001b[39;49m(Image, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data\u001b[39m=\u001b[39;49mdata, url\u001b[39m=\u001b[39;49murl, filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m    971\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata)\n\u001b[1;32m    973\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m, {}):\n\u001b[1;32m    974\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m=\u001b[39m metadata[\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m]\n",
            "File \u001b[0;32m/opt/anaconda3/envs/CV/lib/python3.10/site-packages/IPython/core/display.py:327\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 327\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreload()\n\u001b[1;32m    328\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_data()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/CV/lib/python3.10/site-packages/IPython/core/display.py:1005\u001b[0m, in \u001b[0;36mImage.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[39msuper\u001b[39;49m(Image,\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mreload()\n\u001b[1;32m   1006\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretina:\n\u001b[1;32m   1007\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retina_shape()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/CV/lib/python3.10/site-packages/IPython/core/display.py:353\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     encoding \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_flags \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_flags, encoding\u001b[39m=\u001b[39;49mencoding) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    354\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m    355\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[39m# Deferred import\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/eric/Desktop/2-Career/Projects/ObjectDetection/runs/detect/train_2023.12.05_17.00.11_n_im640_ep64_baNone_seNone/val_batch0_pred.jpg'"
          ]
        }
      ],
      "source": [
        "Image(filename=f'{HOME}/runs/detect/{latest_train_dir}/val_batch0_pred.jpg', width=600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "000ec4a942dc4fceac816501fe6436a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "177f20cb500d4dc7ad338e844897c735": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43df62c36cec4dfaba5203164756f692",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f19fe218fc5940b490dbd9d4dfae89f9",
            "value": " 6/6 [01:25&lt;00:00, 12.37s/it]"
          }
        },
        "23636106a532455c8146205e4f7be2fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43df62c36cec4dfaba5203164756f692": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58be2b02981b4aedad5294cecbccc3be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6250697a80024656af350a4c05d456bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90924baeb3644d45bbfdfb9f7b0520dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1f33b8580474febb5ffe5be8442e090": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec38cea329c64274a2747e7387b8ae3c",
              "IPY_MODEL_c3cff42073644b91863056fe6a2d8f34",
              "IPY_MODEL_177f20cb500d4dc7ad338e844897c735"
            ],
            "layout": "IPY_MODEL_23636106a532455c8146205e4f7be2fe"
          }
        },
        "c3cff42073644b91863056fe6a2d8f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_000ec4a942dc4fceac816501fe6436a7",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90924baeb3644d45bbfdfb9f7b0520dc",
            "value": 6
          }
        },
        "ec38cea329c64274a2747e7387b8ae3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6250697a80024656af350a4c05d456bc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_58be2b02981b4aedad5294cecbccc3be",
            "value": "100%"
          }
        },
        "f19fe218fc5940b490dbd9d4dfae89f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
