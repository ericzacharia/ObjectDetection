{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udhh5dN_Ai54",
        "outputId": "b9fb472b-9ace-4a97-8685-0a1dfb798f3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA not available. Using MPS.\n",
            "CUDA not available. Using MPS.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/Users/eric/Desktop/2-Career/Projects/ObjectDetection'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import supervision as sv\n",
        "from autodistill.detection import CaptionOntology\n",
        "from autodistill_grounded_sam import GroundedSAM\n",
        "# from autodistill_yolov8 import YOLOv8\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from boxmot import (OCSORT, BoTSORT, BYTETracker, DeepOCSORT, StrongSORT,\n",
        "                    create_tracker, get_tracker_config)\n",
        "\n",
        "import video_multiprocessing as vm\n",
        "import superimpose_boxes as sb\n",
        "\n",
        "HOME = os.getcwd()\n",
        "HOME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;36m/Users/eric/Desktop/2-Career/Projects/ObjectDetection/runs/detect/train_2023.12.03_19.33.40_n_im640_ep120_baNone_seNone/\u001b[0m\n",
            "├── args.yaml\n",
            "├── labels.jpg\n",
            "├── labels_correlogram.jpg\n",
            "├── results.csv\n",
            "├── train_batch0.jpg\n",
            "├── train_batch1.jpg\n",
            "├── train_batch2.jpg\n",
            "└── \u001b[1;36mweights\u001b[0m\n",
            "    ├── best.pt\n",
            "    └── last.pt\n",
            "\n",
            "2 directories, 9 files\n"
          ]
        }
      ],
      "source": [
        "# latest_train_dir = input(\"Enter the name of the latest training directory: \")\n",
        "latest_train_dir = 'train_2023.12.03_19.33.40_n_im640_ep120_baNone_seNone'\n",
        "weights = 'best' \n",
        "# # weights = 'last' \n",
        "model = YOLO(f\"runs/detect/{latest_train_dir}/weights/{weights}.pt\")\n",
        "!tree {HOME}/runs/detect/{latest_train_dir}/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BoT-SORT Parameters\n",
        "<b>track_high_thresh</b>: A confidence threshold to determine if a detection should initially be considered for tracking. \n",
        "A higher threshold means only detections with high confidence are tracked, potentially reducing false positives but possibly missing less confident true objects.\n",
        "<br>\n",
        "<b>track_low_thresh</b>: Once an detection is being tracked, this threshold is used in subsequent frames to decide if a detection continues to match with the existing track. A lower threshold is more lenient, which can be useful in maintaining track of objects even when the model's confidence in them dips temporarily.\n",
        "<br>\n",
        "<b>new_track_thresh</b>: This determines how confident the model should be to start tracking (a new detection / a detection that does not match any existing tracks).\n",
        "<br>\n",
        "<b>track_buffer</b>: This is the number of frames an object can be missing or not detected before its track is terminated.\n",
        "<br>\n",
        "<b>match_thresh</b>: This is the threshold for matching detections to the existing tracks between frames. A higher threshold requires detections to be more similar to the existing track, reducing identity switches but potentially losing track more often.\n",
        "<br>\n",
        "<b>gmc_method</b>: The method used to calculate the IOU of two bounding boxes. Can be either 'iou' or 'giou'.\n",
        "make a bullet list of the items below\n",
        "<ul>\n",
        "<li> <b>sparseOptFlow</b>: This setting, standing for Global Motion Compensation, is specific to BoT-SORT.</li>\n",
        "<li> <b>orb</b>: ORB is a fast feature detector and descriptor. It is used for detecting keypoints and computing descriptors that can be matched across frames.\n",
        "ORB is useful for tracking objects in scenarios where you need a balance between speed and accuracy. It's less sensitive to changes in lighting compared to some other methods.</li>\n",
        "<li> <b>sift</b>: SIFT is a more computationally intensive method compared to ORB, used for detecting and describing local features in images.\n",
        "SIFT is highly robust against changes in scale, noise, and illumination, making it suitable for applications where high accuracy is required, even if it means slower processing.</li>\n",
        "<li> <b>ecc</b>: ECC is an algorithm for aligning images (finding the optimal spatial transformation between them). It's particularly good at handling small motions and subtle transformations.\n",
        "ECC can be used for precise alignment of video frames, which is useful in situations where the camera is moving slightly or where very accurate tracking of objects is required.</li>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from datetime import datetime\n",
        "\n",
        "# botsort_contents = '''# BoT-SORT tracker https://github.com/NirAharon/BoT-SORT\n",
        "\n",
        "# # tracker_type: This specifies the type of tracker being used. In your case, 'bytetrack' is selected. \n",
        "# # ByteTrack is known for its efficiency and accuracy in tracking multiple objects even with occlusions or in crowded scenes.\n",
        "\n",
        "# # track_high_thresh: This is the high threshold for the first association in tracking. \n",
        "# # It's a confidence threshold used to determine if an object detected by the YOLO model should initially be considered for tracking. \n",
        "# # A higher threshold means only detections with high confidence are tracked, potentially reducing false positives but possibly missing less confident true objects.\n",
        "\n",
        "# # track_low_thresh: This is the lower threshold for the second association. \n",
        "# # Once an object is being tracked, this threshold is used in subsequent frames to decide if a detected object continues to match with the existing track. \n",
        "# # A lower threshold here allows for more lenient tracking, which can be useful in maintaining track of objects even when the model's confidence in them dips temporarily.\n",
        "\n",
        "# # new_track_thresh: This threshold is used to initiate a new track if a detection does not match any existing tracks. \n",
        "# # It determines how confident the model should be to start tracking a new object. \n",
        "# # A higher value means fewer new tracks will be created, reducing clutter but possibly missing real objects.\n",
        "\n",
        "# # track_buffer: This setting determines the buffer time to calculate when to remove tracks. \n",
        "# # It's essentially the number of frames an object can be missing or not detected before its track is terminated. \n",
        "# # A larger buffer allows tracks to be maintained even when objects are occluded or miss detection for a few frames.\n",
        "\n",
        "# # match_thresh: This is the threshold for matching tracks between frames. \n",
        "# # It's used in the data association step to decide whether a detected \n",
        "# # object in the current frame matches an existing track. \n",
        "# # A higher threshold ensures that only detections very similar to the existing track \n",
        "# # are matched, reducing identity switches but potentially losing track more often.\n",
        "\n",
        "# # gmc_method: This setting, standing for Global Motion Compensation, is specific to BoT-SORT. \n",
        "#     # sparseOptFlow: method compensates for camera motion by using optical flow techniques, \n",
        "#     # helping maintain track stability even with moving cameras.\n",
        "\n",
        "#     # orb: ORB is a fast feature detector and descriptor. It is used for detecting keypoints and computing \n",
        "#     # descriptors that can be matched across frames.\n",
        "#     # ORB is useful for tracking objects in scenarios where you need a balance between speed and accuracy. \n",
        "#     # It's less sensitive to changes in lighting compared to some other methods.\n",
        "\n",
        "#     # sift: SIFT is a more computationally intensive method compared to ORB, used for detecting and describing local features in images.\n",
        "#     # SIFT is highly robust against changes in scale, noise, and illumination, making it suitable for applications where \n",
        "#     # high accuracy is required, even if it means slower processing.\n",
        "\n",
        "#     # ecc: ECC is an algorithm for aligning images (finding the optimal spatial transformation between them). \n",
        "#     # It's particularly good at handling small motions and subtle transformations.\n",
        "#     # ECC can be used for precise alignment of video frames, which is useful in situations where \n",
        "#     # the camera is moving slightly or where very accurate tracking of objects is required.\n",
        "\n",
        "# # proximity_thresh: A threshold for proximity-based matching in tracking. \n",
        "# # It likely influences how close in terms of spatial distance a detected object needs to be across frames to be considered the same object.\n",
        "# # For instance, if the distance metric is based on the relative positions of bounding boxes in a normalized coordinate space \n",
        "# # (like a frame of 1080p video being scaled to a 1x1 square), \n",
        "# # 0.5 could mean that the centers of the bounding boxes are at most half the width/height of the normalized space apart.\n",
        "\n",
        "# # appearance_thresh: This threshold might be related to appearance-based matching, determining how similar in appearance \n",
        "# # a detected object needs to be across frames to maintain the same track. \n",
        "# # This threshold is a normalized value, where 0.0 would represent no requirement for appearance similarity (any object could be matched with any other), \n",
        "# # and 1.0 would represent a requirement for exact appearance matching. \n",
        "# # Thus, a value of 0.25 indicates a relatively lenient requirement for appearance similarity. \n",
        "# # It allows for some variation in how an object appears across frames, which can be useful in dynamic environments where lighting, orientation, or partial occlusions \n",
        "# # might slightly alter an object's appearance.\n",
        "\n",
        "# # with_reid: Indicates whether Re-identification (ReID) features are used. ReID involves using features based on the object’s appearance \n",
        "# # to re-identify and track objects across different camera views or after long periods of occlusion. False indicates it is not currently used.\n",
        "\n",
        "# tracker_type: botsort  # tracker type, ['botsort', 'bytetrack']\n",
        "# track_high_thresh: {track_high_thresh}  # threshold for the first association\n",
        "# track_low_thresh: {track_low_thresh}  # threshold for the second association\n",
        "# new_track_thresh: {new_track_thresh}  # threshold for init new track if the detection does not match any tracks\n",
        "# track_buffer: {track_buffer}  # buffer to calculate the time when to remove tracks\n",
        "# match_thresh: {match_thresh} # threshold for matching tracks\n",
        "# # min_box_area: 10  # threshold for min box areas(for tracker evaluation, not used for now)\n",
        "# # mot20: False  # for tracker evaluation(not used for now)\n",
        "\n",
        "# # BoT-SORT settings\n",
        "# gmc_method: {gmc_method}  # method of global motion compensation [sparseOptFlow, orb, sift, ecc]\n",
        "# # ReID model related thresh (not supported yet)\n",
        "# proximity_thresh: {proximity_thresh}\n",
        "# appearance_thresh: {appearance_thresh}\n",
        "# with_reid: {with_reid}'''\n",
        "# botsort_args = {\n",
        "#     'track_high_thresh': 0.5,  # determines if detection should initially be considered for tracking\n",
        "#     'track_low_thresh': 0.01,  # determines if a detected object continues to match with the existing track\n",
        "#     'new_track_thresh': 0.60,  # determines how confident the model should be to start tracking a new object\n",
        "#     'track_buffer': 99999,  # number of frames an object can be missing or not detected before its track is terminated\n",
        "#     'match_thresh': 0.99, # determines whether a detected object in the current frame matches an existing track\n",
        "#     'gmc_method': 'orb',  # [sparseOptFlow, orb, sift, ecc] Global Motion Compensation Method\n",
        "#     # Below are not supported yet\n",
        "#     'proximity_thresh': 0.95,  # threshold for proximity-based matching in tracking\n",
        "#     'appearance_thresh': 0.91,  # threshold for appearance-based matching in tracking\n",
        "#     'with_reid': True,  # indicates whether Re-identification (ReID) features are used\n",
        "# }\n",
        "# botsort_contents = botsort_contents.format(**botsort_args)\n",
        "# name = f\"Track {botsort_args['track_low_thresh']}<T<{botsort_args['track_high_thresh']} New_{botsort_args['new_track_thresh']} Match_{botsort_args['match_thresh']} GMC_{botsort_args['gmc_method']}\"\n",
        "# videos = {\n",
        "#     1: 'videos/58.mp4',\n",
        "#     2: 'images',\n",
        "#     3: 'hockey/videos/STL_VS_SAJ_003.mp4',\n",
        "#     4: '002.mp4',\n",
        "#     5: '002_trim.mp4',\n",
        "#     6: '002_trim2.mp4',\n",
        "#     7: 'hockey/videos/CHI_VS_TOR_004.mp4',\n",
        "# }\n",
        "# yolo_args = {\n",
        "#     'source': videos[6],\n",
        "#     'device': 'mps',\n",
        "#     'tracker': 'botsort1.yaml',\n",
        "#     'conf': 0.25,\n",
        "#     'iou': 0.3,\n",
        "#     'save': True,\n",
        "#     'save_txt': False,\n",
        "#     'save_conf': True,\n",
        "#     'line_thickness': 1,\n",
        "#     'imgsz': 640,\n",
        "#     'name': name,\n",
        "#     # 'retina_masks': True,\n",
        "# }\n",
        "# weights = 'best' \n",
        "# # weights = 'last' \n",
        "\n",
        "# # write to file botsort1.yaml\n",
        "# with open('botsort1.yaml', 'w') as f:\n",
        "#     f.write(botsort_contents)\n",
        "\n",
        "\n",
        "\n",
        "# print(f'Using the {weights.upper()} weights from runs/detect/{latest_train_dir}\\n')\n",
        "# print(f'Saving to runs/detect/{latest_train_dir}/results\\n')\n",
        "# # model = YOLO(f\"epoch7.pt\")\n",
        "# model = YOLO(f\"runs/detect/{latest_train_dir}/weights/{weights}.pt\")\n",
        "# # results = model.predict(**yolo_args) \n",
        "# # results = model.track(**yolo_args) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "break\n",
        "'''Attempting to integrate Trackers without using Ultralytics below \n",
        "because Ultralytics is not compatible with REID'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_video_writer(video_cap, output_filename):\n",
        "\n",
        "    # grab the width, height, and fps of the frames in the video stream.\n",
        "    frame_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(video_cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # initialize the FourCC and a video writer object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(output_filename, fourcc, fps,\n",
        "                             (frame_width, frame_height))\n",
        "\n",
        "    return writer\n",
        "\n",
        "\n",
        "device = \"mps:0\" # cuda:0 , cpu\n",
        "fp16 = True # True if gpu available\n",
        "source = \"002.mp4\"\n",
        "\n",
        "class_dict = {\n",
        "    # class, color (BGR)\n",
        "    0: ('skater', (0, 30, 255)),\n",
        "    1: ('goalie', (0, 255, 50)),\n",
        "    2: ('referee', (255, 180, 0)),\n",
        "}\n",
        "tracker = DeepOCSORT(\n",
        "    model_weights=Path('osnet_x0_25_msmt17.pt'), # which ReID model to use\n",
        "    device=device,\n",
        "    fp16=fp16,\n",
        ")\n",
        "vid = cv2.VideoCapture(source)\n",
        "writer = create_video_writer(vid, \"DeepOCSORT.mp4\")\n",
        "\n",
        "while True:\n",
        "    ret, im = vid.read()\n",
        "\n",
        "    detections = model.predict(im )[0]\n",
        "\n",
        "    # initialize the list of bounding boxes and confidences\n",
        "    results = []\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if np.array(detections.boxes.data.tolist()).ndim < 2:\n",
        "        results = [[0, 0, 0, 0, 0.0922948837280273, 0]] # --> (x, y, x, y, id, conf, cls)\n",
        "    ts = tracker.update(np.array(detections.boxes.data.tolist()), im) # --> (x, y, x, y, id, conf, cls)\n",
        "\n",
        "    xyxys = ts[:,0:4].astype('int') # float64 to int\n",
        "    ids = ts[:, 4].astype('int') # float64 to int \n",
        "    confs = ts[:, 5]\n",
        "    clss = ts[:, 6]\n",
        "\n",
        "    # print bboxes with their associated id, cls and conf\n",
        "    if ts.shape[0] != 0:\n",
        "        for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n",
        "            im = cv2.rectangle(\n",
        "                im,\n",
        "                (xyxy[0], xyxy[1]),\n",
        "                (xyxy[2], xyxy[3]),\n",
        "                class_dict[cls][1],\n",
        "                1\n",
        "            )\n",
        "            cv2.putText(\n",
        "                im,\n",
        "                f'{class_dict[cls][0]}, confidence: {round(conf, 1)}, id:{id}',\n",
        "                (xyxy[0], xyxy[1]-10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.5,\n",
        "                class_dict[cls][1],\n",
        "                1\n",
        "            )\n",
        "\n",
        "    writer.write(im)\n",
        "\n",
        "\n",
        "vid.release()\n",
        "writer.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# from boxmot import BoTSORT, DeepOCSORT, OCSORT, HybridSORT, StrongSORT\n",
        "# from pathlib import Path\n",
        "\n",
        "# # Initialize your YOLO object detection model\n",
        "# model = YOLO(f\"runs/detect/{latest_train_dir}/weights/{weights}.pt\")\n",
        "\n",
        "# # Initialize the DeepOCSORT tracker\n",
        "# tracker = BoTSORT(\n",
        "#     model_weights=Path('osnet_x0_25_msmt17.pt'), # ReID model\n",
        "#     device='mps',\n",
        "#     fp16=True,\n",
        "#     # img=None,\n",
        "# )\n",
        "# yolo_args_frame = {\n",
        "#     'device': 'mps',\n",
        "#     'tracker': 'botsort1.yaml',\n",
        "#     'conf': 0.25,\n",
        "#     'iou': 0.3,\n",
        "#     'save': True,\n",
        "#     'save_txt': False,\n",
        "#     'save_conf': True,\n",
        "#     'line_thickness': 1,\n",
        "#     'imgsz': 640,\n",
        "#     'name': name,\n",
        "#     # 'retina_masks': True,\n",
        "# }\n",
        "\n",
        "# video_path = yolo_args['source']\n",
        "# vid = cv2.VideoCapture(video_path)\n",
        "# # vid = cv2.VideoCapture(0)\n",
        "# color = (0, 0, 255)  # BGR\n",
        "# thickness = 2\n",
        "# fontscale = 0.5\n",
        "\n",
        "# def process_detections(results):\n",
        "#     detections = []\n",
        "#     for r in results:\n",
        "#         for xyxy, confidence, classification in zip(r.boxes.xyxy.cpu(), r.boxes.conf.cpu(), r.boxes.cls.cpu()):\n",
        "#             # Convert tensor to numpy array and round off\n",
        "#             bbox = xyxy.numpy().astype(int).tolist()\n",
        "#             detections.append((*bbox, confidence.item(), classification.item()))\n",
        "#     # Convert detections to a numpy array\n",
        "#     return np.array(detections)\n",
        "\n",
        "# while True:\n",
        "#     ret, im = vid.read()\n",
        "\n",
        "#     results = model.predict(source=im, **yolo_args_frame) \n",
        "#     dets = process_detections(results)\n",
        "\n",
        "#     try:\n",
        "#         tracks = tracker.update(dets, im) # --> (x, y, x, y, id, conf, cls, ind)\n",
        "#     except:\n",
        "#         # print(im.shape)\n",
        "#         continue\n",
        "#         # vid.release()\n",
        "#         # cv2.destroyAllWindows()\n",
        "\n",
        "#     xyxys = tracks[:, 0:4].astype('int') # float64 to int\n",
        "#     ids = tracks[:, 4].astype('int') # float64 to int\n",
        "#     confs = tracks[:, 5]\n",
        "#     clss = tracks[:, 6].astype('int') # float64 to int\n",
        "#     inds = tracks[:, 7].astype('int') # float64 to int\n",
        "\n",
        "#     # print bboxes with their associated id, cls and conf\n",
        "#     if tracks.shape[0] != 0:\n",
        "#         for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n",
        "#             im = cv2.rectangle(\n",
        "#                 im,\n",
        "#                 (xyxy[0], xyxy[1]),\n",
        "#                 (xyxy[2], xyxy[3]),\n",
        "#                 color,\n",
        "#                 thickness\n",
        "#             )\n",
        "#             cv2.putText(\n",
        "#                 im,\n",
        "#                 f'id: {id}, conf: {conf}, c: {cls}',\n",
        "#                 (xyxy[0], xyxy[1]-10),\n",
        "#                 cv2.FONT_HERSHEY_SIMPLEX,\n",
        "#                 fontscale,\n",
        "#                 color,\n",
        "#                 thickness\n",
        "#             )\n",
        "\n",
        "#     # show image with bboxes, ids, classes and confidences\n",
        "#     cv2.imshow('frame', im)\n",
        "\n",
        "#     # break on pressing q\n",
        "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "#         break\n",
        "\n",
        "# vid.release()\n",
        "# cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# from boxmot import BoTSORT, DeepOCSORT, OCSORT, HybridSORT, StrongSORT\n",
        "# from pathlib import Path\n",
        "\n",
        "# # Initialize your YOLO object detection model\n",
        "# model = YOLO(f\"runs/detect/{latest_train_dir}/weights/{weights}.pt\")\n",
        "\n",
        "# # Initialize the DeepOCSORT tracker\n",
        "# tracker = BoTSORT(\n",
        "#     model_weights=Path('osnet_x0_25_msmt17.pt'), # ReID model\n",
        "#     device='mps',\n",
        "#     fp16=True,\n",
        "#     # img=None,\n",
        "# )\n",
        "# yolo_args_frame = {\n",
        "#     'device': 'mps',\n",
        "#     'tracker': 'botsort1.yaml',\n",
        "#     'conf': 0.25,\n",
        "#     'iou': 0.3,\n",
        "#     'save': True,\n",
        "#     'save_txt': False,\n",
        "#     'save_conf': True,\n",
        "#     'line_thickness': 1,\n",
        "#     'imgsz': 640,\n",
        "#     'name': name,\n",
        "#     # 'retina_masks': True,\n",
        "# }\n",
        "\n",
        "# def process_detections(results):\n",
        "#     detections = []\n",
        "#     for r in results:\n",
        "#         for xyxy, confidence, classification in zip(r.boxes.xyxy.cpu(), r.boxes.conf.cpu(), r.boxes.cls.cpu()):\n",
        "#             # Convert tensor to numpy array and round off\n",
        "#             bbox = xyxy.numpy().astype(int).tolist()\n",
        "#             detections.append((*bbox, confidence.item(), classification.item()))\n",
        "#     # Convert detections to a numpy array\n",
        "#     return np.array(detections)\n",
        "\n",
        "\n",
        "# def main(video_path, model, tracker):\n",
        "#     # Open the video\n",
        "#     cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "#     if not cap.isOpened():\n",
        "#         print(\"Error: Could not open video.\")\n",
        "#         return\n",
        "\n",
        "#     # Get video properties for the output file\n",
        "#     frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "#     frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "#     # Define the codec and create VideoWriter object\n",
        "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or 'XVID' if 'mp4v' does not work\n",
        "#     out = cv2.VideoWriter('output_video.mp4', fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "#     while cap.isOpened():\n",
        "#         ret, frame = cap.read()\n",
        "#         if not ret:\n",
        "#             break\n",
        "\n",
        "#         # Process frame with YOLO model to get results\n",
        "#         results = model.predict(source=frame, **yolo_args_frame) \n",
        "\n",
        "#         # Process detections\n",
        "#         detections = process_detections(results)\n",
        "\n",
        "#         try:\n",
        "\n",
        "#             # Update the tracker with the new detections\n",
        "#             tracked_objects = tracker.update(detections, frame)\n",
        "\n",
        "#         # Draw tracked objects on the frame\n",
        "#             for obj in tracked_objects:\n",
        "#                 # Draw a rectangle around the object\n",
        "#                 # print(obj, obj[0], obj[1], obj[2], obj[3], type(obj[0]), type(obj[1]), type(obj[2]), type(obj[3]))\n",
        "#                 cv2.rectangle(frame, (int(obj[0]), int(obj[1])), (int(obj[2]), int(obj[3])), (0, 255, 0), 2)\n",
        "                \n",
        "#                 # Draw the object ID near the object\n",
        "#                 cv2.putText(frame, str(obj[4]), (int(obj[0]), int(obj[1]) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "#             out.write(frame)\n",
        "#         except:\n",
        "#             pass\n",
        "#     # Release everything when job is finished\n",
        "#     cap.release()\n",
        "#     out.release()\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     video_path = yolo_args['source']\n",
        "#     main(video_path, model, tracker)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "000ec4a942dc4fceac816501fe6436a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "177f20cb500d4dc7ad338e844897c735": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43df62c36cec4dfaba5203164756f692",
            "placeholder": "​",
            "style": "IPY_MODEL_f19fe218fc5940b490dbd9d4dfae89f9",
            "value": " 6/6 [01:25&lt;00:00, 12.37s/it]"
          }
        },
        "23636106a532455c8146205e4f7be2fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43df62c36cec4dfaba5203164756f692": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58be2b02981b4aedad5294cecbccc3be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6250697a80024656af350a4c05d456bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90924baeb3644d45bbfdfb9f7b0520dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1f33b8580474febb5ffe5be8442e090": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec38cea329c64274a2747e7387b8ae3c",
              "IPY_MODEL_c3cff42073644b91863056fe6a2d8f34",
              "IPY_MODEL_177f20cb500d4dc7ad338e844897c735"
            ],
            "layout": "IPY_MODEL_23636106a532455c8146205e4f7be2fe"
          }
        },
        "c3cff42073644b91863056fe6a2d8f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_000ec4a942dc4fceac816501fe6436a7",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90924baeb3644d45bbfdfb9f7b0520dc",
            "value": 6
          }
        },
        "ec38cea329c64274a2747e7387b8ae3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6250697a80024656af350a4c05d456bc",
            "placeholder": "​",
            "style": "IPY_MODEL_58be2b02981b4aedad5294cecbccc3be",
            "value": "100%"
          }
        },
        "f19fe218fc5940b490dbd9d4dfae89f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
